\subsection{ \textbf{Monte Carlo Transport on a GPU}}

This section will analyze the different approaches Monte Carlo transport codes utilizing GPU architectures.
%
It will begin by comparing and contrasting different approaches by evaluating a few key areas of the studies that have been done: accuracy, performance and algorithmic choices.
%
Following will be an evaluation of the effectiveness of the approaches for the range of problems being addressed.
%
As a side note it is important to notice that speedups reported come from each paper on the hardware they were using at the time of their study; the GPU hardware has changed in computing power dramatically over the last ten years in terms of performance and additional features.

\subsection*{ \textbf{Accuracy:} }

One of the first considerations the scientific community has when being introduced to a new computing platform is what levels of accuracy can they achieve with their simulation codes.
%
Since the change from CPU to GPU computing brings a completely different hardware design it is important to understand how that design might affect the accuracy of any calculations it is performing.
%
This concern was especially important in the early days of GPU computing when double-precision was not supported and often even single-precision answers would provide slightly different results.
%
There are three key areas of accuracy to consider: Floating point precision, differences between CPU and GPU results, and IEE-754 compliance.
%

%
It was commonly assumed in the early stages of GPU computing that accuracy was lacking.
%
Many early attempts at GPU computing include discussions of accuracy in order to validate the correctness of their results.
%
While modern GPGPUs support double-precision much better than before, making much of the worry irrelevant, it is still important to consider the accuracy of a method that runs on a new hardware and may use a new algorithm.
%

\subsubsection*{\textbf{Floating Point Accuracy}}

One of the primary concerns of the early GPU studies involved understanding the limits of floating point arithmetic on the GPU architecture.
%
Nelson~\cite{nelson2009monte} describes one of his primary accuracy considerations as being the difference between single and double-precision calculations.
%
In older GPU hardware there was no support for double-precision in the hardware and so in order to achieve double-precision significantly more calculations were needed.
%
In modern GPU hardware 64 bit double-precision is becoming increasingly better supported and in the GPGPU cards there are dedicated double-precision units.
%

\subsubsection*{\textbf{Differences Between CPU and GPU results}}

An even larger concern than the differences between single and double-precision is differences in results that arise when using the same precision but on different architectures.
%
This concern can be understood by considering how floating-point math is accomplished on a computer~\cite{goldberg1991every}.
%
There are two main reasons that differences arise.
%
The first is that floating point mathematical operations performed in different orders might produce different results and, due to the nature of parallel computing, the ordering of these calculations is not guaranteed.
%
The second reason is that modern day CPUs using x86 processors perform math internally on 80 bit registers while a GPU does it on 32 bit (single-precision) or 64 bit (double-precision) registers.
%
Because of this, each math operation on a CPU might stay in registers and only be rounded down to 64 bits when it is saved to memory.
%

%
Jia et al. ~\cite{jia2010development} showed that in their development of a Monte Carlo dose calculation code they could achieve speedups of 5 to 6.6 times over their CPU version while maintaining within 1\% of the dosing for more than 98\% of the calculation points.
%
They considered this adequate accuracy to consider using GPUs for doing these computations.
%
Yepes et al.~\cite{yepes2010gpu} also considered accuracy in their assessment of their GPU implementation.
%
They concluded that, in terms of accuracy, there was a good agreement between the dose distributions calculated with each version they ran, with the largest discrepancies being only $\sim$3\%, and so they could run the GPU version as accurately as any general-purpose Monte Carlo program.
%
As these two groups have shown, this amount of error is often very small, and over the entire course of the simulation only brings 1-3\% errors.

\subsubsection*{\textbf{IEE-754 Compliance}}
Nelson discussed accuracy in his thesis work~\cite{nelson2009monte} as floating-point arithmetic accuracy was not fully IEEE-754 compliant during the time of his work.
%
Additionally, since NVIDIA has complete control over the implementation of floating point calculations on their GPUs there may be differences between generations that mitigate the usefulness of an accuracy study on one generation of hardware.
%
Current generations of the NVIDIA GPU hardware are IEEE-754 compliant however. 
%
In order to address issues of floating point accuracy they have even included a detailed description of the standard and the way CUDA follows the standard~\cite{cudaToolkitv7.5}.
So,  while floating point accuracy is still a concern, it is now no more a concern than it was on a CPU implementation.

\subsection*{\textbf{Performance:}}

Performance is a second important factor for Monte Carlo transport on GPUs.
%
Most early GPU studies emphasize their speedups over CPUs as the primary advantage for moving over to the GPU hardware.
%
Given the change in supercomputing designs these comparisons have become increasingly more important.
%

%
Often, performance is compared to the hardware maximums such as peak of FLOPS or memory bandwidth.
%
It is often assumed that an increase in available FLOPS will translate directly into incredible performance gains.
%
In Lee et al.'s Debunking the 100X GPU vs. CPU myth~\cite{lee2010debunking}, this discussion of performance is brought into new light showing the relative performance gains for different types of applications.
%
The important thing to consider is the limiting factor between the hardware and the code.
%
As a result, comparing current performance with that of peak performance is often very misleading.
%

%
The following discussions show the relative performances of Monte Carlo transport applications that either underwent a transformation to use GPUs or performed a study comparing with GPU hardware.
%
We will not see the 100x performance that is often sought after, but instead we can understand the impact that each applications problem, algorithms, and implementation differences had on the performance as a whole.
%

\subsubsection*{\textbf{Photon Transport}}
%
Badal and Badano~\cite{badal2009accelerating} present work on photon transport in a voxelized geometry showing results around 27X over a single core CPU.
%
Their work emphasizes simply using GPUs instead of CPUs as GPUs increase performance faster than CPUs.
%

%
Ren et al.~\cite{ren2010gpu} present work on photon propagation through tissue, showing around a 10X performance increase when using the GPU.
%
Their discussion expressed clearly that the performance was related strongly to the size of the data set and the number of simulated photons.
%
In addition, their results were negatively affected by high level divergence when processing different types of tissues.
%

%
Alerstam et al.~\cite{alerstam2008parallel} present work on a GPU based photon migration simulation in CUDA with speedups around 1000X over a single core CPU.
%
This specific problem does not suffer from the same divergence issues that other Monte Carlo codes have as the algorithm for completing a photon migration has very little divergence and can be easily optimized for memory layouts.
%
However, the 1000X speedup discussed here does not cover the entire application and ignores many factors that limit total speedup due to Amdahl's Law effects.
%

\subsubsection*{\textbf{Neutron Transport}}

%
Nelson's work presented in his thesis~\cite{nelson2009monte} shows a variety of models and considerations for his performance results.
%
His work solving neutron transport considered multiple models for running the problem and optimizing for the GPU.
%
The model that produced his best results shows 19X from a 49,152 neutrons per batch run for single-precision.
%
The same model shows 23X when using single-precision and fast math.
%
For double-precision performance the fastest speedups he observed were 12X.
%

Work done by Gong et al.~\cite{gong2011accelerating} in a MCNP-based application has similar performance benefits to Nelson's work.
%
Speedup factors of 16X to 23X were found depending on problem parameters.
%
This work was only an introductory attempt at implementing MCNP in CUDA, as MCNP is so large that it is time intensive to consider more than a subset of possible features and problem types.
%

Heimlich et al.~\cite{heimlich2009gpu} reported a speedup of around 15X for his neutron transport application when comparing a GPU to an 8-core CPU.
%
This work focused on optimizing a history-based approach in CUDA for the GPU and using MPI+OpenMP for the CPU.
%
This particular algorithm contained only small amounts of divergence in the code path that computes the random walk of neutrons, providing a possibility for greater use of available parallelism.
%


\subsubsection*{\textbf{Gamma Ray Transport}}
%
Work presented by Tickner~\cite{tickner2010monte} on X-ray and gamma ray transport uses a slightly modified scheme from the others by launching particles on a per block basis.
%
In this way, he hoped to remove the instruction-level dependencies between particles running on the GPU.
%
In this work, he produced speedups of up to 35X over a single core CPU, a significant improvement over similar methods launching with a particle-per-thread scheme.
%

\subsubsection*{\textbf{Coupled Electron Photon Transport}}
\label{sec:firstPassCoupledElectronPhoton}
%
Jia et al.'s  work~\cite{jia2010development} in a dose calculation code for coupled electron photon transport follows a relatively straight-forward algorithm.
%
In their work, they offload the data and computations to the GPU, simulate the particles, and then copy memory back.
%
This method produced a modest performance increase on a GPU of around 5 to 6.6X over their runs on a CPU.
%
The limitation of this speedup was attributed to the branching of the code.

\subsubsection*{\textbf{Track Repeating Algorithm}}
In contrast to Jia et al.'s work, Yepes et al.~\cite{yepes2010gpu} showed that a different algorithm could greatly improve results.
%
By converting a track-repeating algorithm instead of a full physics Monte Carlo code, Yepes et al. gained around 75X the performance on the GPU over the CPU.
%
It is thought that the simpler logic of this algorithm generated threads which followed less branching paths than the algorithm presented in Jia et al.'s work.

\subsubsection*{\textbf{Performance Evaluation}}
All of these examples contain a common theme.
%
While performance can be gained doing Monte Carlo on the GPU, it can be more difficult to get than expected due to the highly divergent nature of the Monte Carlo algorithm.
%
Methods to deal with this divergence show promising results.
%
These outcomes are expected since Monte Carlo applications are embarrassingly parallel (good for GPUs) but also incredibly divergent (bad for GPUs).
%

In this section, we have seen a wide range in performances, from as low as 5X to as high as 75X, or even 1000X.
%
While simplifications played a large role in the 75X algorithm we do see a full Monte Carlo application achieving speeds of 35X in the case of the work by Tickner~\cite{tickner2010monte}.
%
It is important to note that while some of the differences in performance are due to the nature of each problem being solved, the algorithmic choices made can have a significant impact on the GPU implementations.
%

\subsection*{\textbf{Algorithms:}}

Based on the performance studies we have just seen, it is important to highlight the algorithmic approaches that were taken so that we can understand the performance of each approach.
%
If we can clearly find algorithms that show positive performance results, then other codes can implement them for potential gain.
%
In this section, we are going to look at a few of the important algorithms.
%

%
Monte Carlo transport applications tend to follow a simple model where each tracked particle is given its own thread and computation progresses in an embarrassingly parallel fashion. 
%
On a GPU, this also makes sense as a starting point since particles are independent and this progression leads to a naturally parallel approach.
%
It is often pointed out, however, that due to the divergent nature of Monte Carlo this approach might not be the best way organize Monte Carlo codes on GPU hardware.
%

\subsubsection*{\textbf{Particle-Per-Block}}
We will first look at an alternative approach, the particle-per-block tracking algorithm described by Tickner~\cite{tickner2010monte}.
%
First each tracked particle or quantum of radiation is given to a block of threads.
%
Then calculations are performed for one particle on each block of threads.
%
For example the particle intersection tests with the background geometry can be performed in parallel on those threads for each piece of geometry that particle might be able to collide with.
%
Areas where these parallel instructions can be utilized within a particle's calculation are then used by the threads in a block computing for that particle.
%

This particle-per-block technique is effective in mitigating the divergence issue.
%
Particles often diverge quickly from one another in the code paths they follow.
%
This means that threads in a block are not always able to travel in lock step and can cause some serialization of the parallel regions.
%
By using only one particle per block, the divergence problem is nearly removed from the equation.
%
Additionally, this method introduces a new area of parallelism that is not otherwise being taken advantage of: instruction-level parallelism in the calculations for a single particle.
%

%
This method, however, does not take full advantage of the parallelism in the hardware like the methods that are not sensitive to divergence.
%
Many threads can execute simultaneously within a block with potential slowdowns coming from when groupings of 32 threads are held in a warp and forced into the lockstep pattern.
%
Running only one particle per block can sacrifice some parallelism, as not all tasks to calculate a particle's path are parallel operations.
%
Additionally, since warps are scheduled out of thread blocks, any particle operations that are not done in parallel among the threads of a block are serializing themselves in a similar manner as to those algorithms that run one thread per particle and contain high levels of divergence.
%

In summary, this method has some merit if it can find enough parallel work in the thread block to execute additional parallel tasks that would otherwise be stalled when following a simpler method.
%
Also, this method might end up showing the same characteristics of the simpler particle-per-thread model if the extra parallelism is not found, and instead lose out on the parallelism provided by particles that are not highly divergent from one another. 

\subsubsection*{\textbf{Event-Based Approaches}}

A second, possibly more obvious method, to escape the divergence issue is to switch particle tracking algorithms more dramatically from a history based version to an event based version.
%
Section~\ref{sec:EventBased} discusses this topic later in this paper.
%
Event based approaches require much more work then simply transforming an existing code to use the history based version on the GPU.
%
And as Du et al. discovered in their attempt at an event based Monte Carlo version of the Archer code~\cite{xu2015archer}~\cite{du2013evaluation}~\cite{liu2015comparison}~\cite{su2013monte},  getting any speedups with that method creates a whole new host of challenges to overcome: divergence, atomic operations, data locality/layouts, and portability to name a few.

\subsubsection*{\textbf{Voxelization Approaches}}

This method was used for comparing workloads on GPUs and is often done to voxelize geometries to enable Monte Carlo codes to run.
%
Voxelization of a geometry is done on a voxel-by-voxel basis.
%
This process involved ray-stabbing numbers counted on the GPU, and then using a parity-counting method on the CPU to detect if the voxel was inside the mesh surface~\cite{na2010deformable}.
%
This method contains no divergence since all threads follow the exact same code paths.
%
Doing this algorithm with no divergence produces a 45.5x speedup on the GPU over the CPU.
%
This example is in Ding et al.'s evaluation report~\cite{ding2011evaluation} in order to show the performance of the same GPU on different aspects related to Monte Carlo transport.

\subsection*{ \textbf{Evaluation:}}

\begin{table}
\caption { GPU speedup evaluation results~\cite{ding2011evaluation} } \label{tab:GPUPerfEval} 
\begin{center}
\begin{tabular}{ |C{.10\textwidth}|C{.10\textwidth}|C{.10\textwidth}|C{.10\textwidth}|}
\hline
 & & & \\
Case & Execution Time $T_{CPU}$ (minutes) & Execution Time $T_{GPU}$ (minutes) & Speed-up Factor $T_{CPU}/T_{GPU}$ \\
 & & & \\
 \hline
 & & & \\
Neutron Transport Problem & $~ 0.496$ & $ ~0.017$ & $~ 29.2$ \\ 
 & & & \\
\hline
 & & & \\
Eigenvalue/Criticality Problem & $ 4.25 $ & $ ~ 0.5 $ & $ ~ 8.5 $ \\
 & & & \\
\hline
 & & & \\
Voxelization & $ 2380.4 $ & $ ~ 52.3 $ & $~ 45.5 $ \\
 & & & \\
\hline
\end{tabular}
\end{center}
\end{table}

A number of studies were conducted by groups identifying the potential benefits of GPU hardware but also software development issues with Monte Carlo applications. 
%
Among these concerns are memory limitations, lack of ECC support, lack of software optimization, limitations of SIMD architecture, clock speeds, and complex memory allocation schemes.
%
In addition, the achieved performance often did not exceed that of unchanged codes on a cluster.
%
In some cases, though, speedups were large and easy to achieve, such as the 45X speedup of the voxelized approach. 
%
The results from Ding et al.'s~\cite{ding2011evaluation} evaluations can be seen in Table~\ref{tab:GPUPerfEval}.
%
The only strong conclusion from these works are that a clear and defined path are not yet known on how to take full advantage of the available parallelism without suffering performance penalties in turn.
