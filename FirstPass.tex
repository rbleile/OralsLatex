\subsection{ \textbf{First Pass at GPU Computing}}

In this section we will analyze the different approaches that people have taken to get their Monte Carlo transport codes working for GPUs.
%
By comparing and contrasting these different approaches we can see for a variety of problems if any approaches were more successful and which approaches struggled to get desired levels of performance out of the hardware.
%
 It is important to note that the actual runtimes and speedups are less important then the theoretical speedups compared to the achieved speedups as hardware has been changing very quickly over the last ten years for these comparisons.
 %

\subsection*{ Accuracy: }

One of the first considerations the scientific community has when being introduced to a new computing platform is what levels of accuracy can they achieve with their simulation codes.
%
It was common that in early GPU computing when double floating point precision was not supported or supported well that people started thinking about GPU computing as not being accurate enough for their needs.
%
Many early attempts at GPU computing includes discussions of accuracy in order to validate the correctness of their results.
%
While modern GPGPUs support double precision much better than before making much of the worry irrelevant, it is still important to consider the accuracy of a method that runs on a new hardware and may use a new algorithm.

%
Nelson discussed accuracy in his thesis work ~\cite{nelson2009monte}, stating that during the time of his work the floating-point arithmetic accuracy was not fully IEEE-754 compliant which opens the question of accuracy without a more fully featured test. Additionally, since NVIDIA as complete control over the implementation of floating point calculations on their GPUs their may be differences between generations that mitigate the usefulness of an accuracy study on one generation of hardware.
%
Current generations of the NVIDIA GPU hardware are IEE-754 compliant. 
%
In order to address issues of floating point accuracy they have even included a detailed description of the standard and the way CUDA follows the standard showing that at least while floating point accuracy is still a concern it is no more a concern than it was on a CPU implementation.~\cite{cudaToolkitv7.5}
%
Nelson's other primary accuracy consideration was the difference in computation time between double and single precision.
%
In older GPU hardware there was no support for double precision in the hardware and so in order to achieve double precision significantly more calculations were needed.
%
In modern GPU hardware double precision is becoming increasingly better supported and in the gpgpu cards there are dedicated double precision units and all of the necessary hardware changes required to include them.
%

Others also wanted to consider the accuracy of their initial gpu conversions.
%
Jia et. al ~\cite{jia2010development} showed that in their development of a Mont Carlo dose calculation code they could achieve speedups of 5 to 6.6 times their CPU version while maintaining within 1\% of the dosing for more then 98\% of the calculation points.
%
They considered this adequate accuracy to consider using GPUs for doing these computations.
%
Yepes et. al~\cite{yepes2010gpu} also considered accuracy in their assessment of their GPU implementation.
%
They concluded that in term of accuracy there was a good agreement between the dose distributions calculated with each version they ran, the largest discrepencies being only $\sim$3\%, and so they could run the GPU version as accurately as any general-purpose Monte Carlo program.
%

\subsection*{Performance:}

A second factor that is important to people making their first pass at GPU Monte Carlo is performance.
%
Most early GPU studies emphasize the speedups between CPU and GPU as the primary advantage for moving over to the GPU hardware.
%
Given the change in supercomputing designs these comparisons have become increasingly more important.
%

Badal and Badano~\cite{badal2009accelerating} present work on photon transport in a voxelized geometry showing results around 27$\times$ over a single core CPU.
%
Their work emphasizes simply using GPUs instead of CPUs and the advanatage as GPUs continue to increase in performance faster than CPUs.
%

Nelsons work presented in his thesis~\cite{nelson2009monte} shows a variety of models and considerations for his performance results.
%
His work solving neutron transport considered multiple models for running the problem and optimizing for the GPU.
%
The model that produced his best results shows 19.37$\times$ from a 49,152 neutrons per batch run for single precision.
%
The same model shows 23.91$\times$ when using single precision and fast math,
%
For double precision performance the model labeled model four had the fastest speedups with 11.21$\times$ and 12.66$\times$ with fast math.

Work presented by Tickner~\cite{tickner2010monte} on X-ray and gamma ray transport uses a slightly modified scheme from the others by launching particles on a per block basis.
%
In this way he hoped to remove the instruction level dependancies between particles running on the GPU hardware.
%
In this work he showed he was capable of producing speedups of up to 35$\times$ over those running the old algorithm.

Jia et. al's  work~\cite{jia2010development} in a dose calculation code for coupled electron photon transport follows a relatively straight forward algorithm.
%
First they copy the necessary data to the GPU, then they launch with each thread independently computing the necessary work for their particle.
%
Finally, when the correct number of particles has been simulated the results are brought back to the CPU and the program terminates.
%
This method produced a modest performance increase of around 5 to 6.6 $\times$ their runs on a CPU when run on a GPU.
%
The limitation of this speedup was attributed to the branching of the code and that effect it had on the GPU hardware.

In contrast to Jia et al's work Yepes et al~\cite{yepes2010gpu} showed that a slightly different algorithm could greatly improve results.
%
By converting a track-repeating algorithm instead of a full Monte Carlo Yepes et al gained aroung 75$\times$ the performance on the GPU over the CPU.
%
It is thought that the simpler logic of this algorithm generated threads which followed closer logic to that of the algorithm presented in Jia at al's work.

Throughout all of these examples one common theme can be seen.
%
Performance can be gained doing Monte Carlo on the GPU.
%
Performance can be more difficult to get due to the highly divergent nature of the full Monte Carlo application.
%
Methods to deal with this divergence can show promising results that are worthy of further study.
%
These outcomes are expected outcomes since Monte Carlo applications embarrassingly parallel (a good sign for GPUs ) but also incredibly divergent ( a bad sign for GPUs ).

\subsection*{Approaches: }

Monte Carlo transport applications tend to follow a simple model where each tracked particle is given its own thread and computations progress embarrassingly parallel. 
%
On a GPU this also makes sense as a starting point since particles are independent and this progression leads to a simple natural parallel approach.
%
It is often pointed out however that due to the divergent nature of Monte Carlo this approach might not be the best way organize Monte Carlo codes on GPU hardware.
%

We mentioned in the previous section one persons approach at solving this dilemma and their performance gains by dealing with this issue.
%
We will now look closer at the algorithm they used to better understand why if at all their approach should afford better performance than what was seen before.
%

We will first look at the particle-per-block tracking algorithm described by Tickner~\cite{tickner2010monte}.
%
First each tracked particle or quantum of radiation is given to a block of threads.
%
Then calculations are performed for one particle on each block of threads.
%
For example the particle intersection tests with the background geometry can be preformed in parallel on those threads for each piece of geometry that particle might be able to collide with.
%
Areas where these parallel instructions can be utilized within a particles calculation are then used by the threads in a block computing for that particle.
%

This particle-per-block technique has shows promise as an effective way to counter act the divergence issue.
%
Positives, as they also described, particles often diverge quite quickly from one another in the code paths they follow.
%
This means that threads in a block are not always able to travel in lock step and can cause some serialization of the parallel regions.
%
By using only one particle per block the divergence problem is nearly entirely removed from the equation.
%
Additionally this method introduces new areas of parallelism that are not otherwise being taken advantage of, instruction level parallelism in the particles calculations.
%

%
This method however, does not take full advantage of the parallelism in the hardware like those methods that do not mind the divergence do.
%
Many threads can execute simultaneously at once within a block and only groupings of 32 threads are held in a WARP forced into the lockstep pattern that causes potential slowdowns.
%
By running only one particle per block you are sacrificing some parallelism as not all tasks to calculate a particles path are parallel operations.
%
Additionally, since warps are scheduled out of thread blocks any particle operations that are not done in parallel among the threads of a block are serializing themselves in a similar manner as to those algorithms that run one thread per particle waiting while divergent particles have a turn.
%

In summary I think that this method has some merit if it can find enough parallel work in the thread block to execute additional parallel tasks that would otherwise be stalled if following a simpler method.
%
I also think that this method might end up showing the same characteristics of the simpler particle-per-thread model if the extra parallelism is not found, and instead loose out on the parallelism provided by particles that are not divergent from one another. 

A second possibly more obvious method to escape the divergence issue is to switch particle tracking algorithms more dramatically from a history based version to an event based version.
%
We will have a discussion of this further in the Section on event based algorithms later in this paper.
%
Event based approaches require much more work then simply transforming an existing code to use the history based version on the GPU.
%
And as Du et al discovered in their attempt at a event based Monte Carlo version of the Archer code~\cite{xu2015archer}~\cite{du2013evaluation}  getting any speedups with that method is a whole new host of challenges to overcome.

%%
\subsection*{ Abstracts: }
\subsubsection*{The Following Text Is not complete... it is abstracts and summaries to be made into a complete story. }
%
 
 %
 
The history of Monte Carlo methods is closely linked to that of computers: The first known Monte Carlo ~rogram
was written in 1947 for the EN lAC; a pre-release of the first Fortran compiler was used for Mon~e Carlo In 1957;
Monte Carlo codes were adapted to vector computers in the I 980s, clusters and parallel computers In the 199.0s, and
teraflop systems in the 2000s. Recent advances include hierarchical parallelism, combini.ng threaded calculatIOns on
multicore processors with message-passing among different nodes. WIth. the advances In computmg, Monte Carlo
codes have evolved with new capabilities and new ways of use. ProductIon codes such as MCNP, MYP, MONK,
TRIPOLI and SCALE are now 20-30 years old (or more) and are very rich in advanced featUres. The former
"method 'Of last resort" has now become the first choice for many applications. Calculations are now routmely
performed on office computers, not just on supercomputers. Current research and development efforts ~re
investigating the use of Monte Carlo methods on FPGAs. GPUs, and many-core processors. Other far-reachmg
research is exploring ways to adapt Monte Carlo methods to future exaflop systems that may have 1 M or more
concurrent computational processes.
 ~\cite{brown2011recent}
 %

 %
 Monte Carlo simulation is ideally suited for solving Boltzmann neutron transport equation in
inhomogeneous media. However, routine applications require the computation time to be reduced to hours
and even minutes in a desktop system. The interest in adopting GPUs for Monte Carlo acceleration is
rapidly mounting, fueled partially by the parallelism afforded by the latest GPU technologies and the
challenge to perform full-size reactor core analysis on a routine basis. In this study, Monte Carlo codes for
a fixed-source neutron transport problem and an eigenvalue/criticality problem were developed for CPU
and GPU environments, respectively, to evaluate issues associated with computational speedup afforded by
the use of GPUs. The results suggest that a speedup factor of 30 in Monte Carlo radiation transport of
neutrons is within reach using the state-of-the-art GPU technologies. However, for the
eigenvalue/criticality problem, the speedup was 8.5. In comparison, for a task of voxelizing unstructured
mesh geometry that is more parallel in nature, the speedup of 45 was obtained. It was observed that, to date,
most attempts to adopt GPUs for Monte Carlo acceleration were based on naïve implementations and have
not yielded the level of anticipated gains. Successful implementation of Monte Carlo schemes for GPUs
will likely require the development of an entirely new code. Given the prediction that future-generation
GPU products will likely bring exponentially improved computing power and performances, innovative
hardware and software solutions may make it possible to achieve full-core Monte Carlo calculation within
one hour using a desktop computer system in a few years. 
 ~\cite{ding2011evaluation}
 %
 
  %
  An electron-photon coupled Monte Carlo code ARCHER - Accelerated Radiation-transport Computations in Heterogeneous Environments - is being developed at Rensselaer Polytechnic Institute as a software test bed for emerging heterogeneous high performance computers that utilize accelerators such as GPUs. In this paper, the preliminary results of code development and testing are presented. The electron transport in media was modeled using the class-II condensed history method. The electron energy considered ranges from a few hundred keV to 30 MeV. Moller scattering and bremsstrahlung processes above a preset energy were explicitly modeled. Energy loss below that threshold was accounted for using the Continuously Slowing Down Approximation (CSDA). Photon transport was dealt with using the delta tracking method. Photoelectric effect, Compton scattering and pair production were modeled. Voxelised geometry was supported. A serial ARHCHER-CPU was first written in C++. The code was then ported to the GPU platform using CUDA C. The hardware involved a desktop PC with an Intel Xeon X5660 CPU and six NVIDIA Tesla M2090 GPUs. ARHCHER was tested for a case of 20 MeV electron beam incident perpendicularly on a water-aluminum-water phantom. The depth and lateral dose profiles were found to agree with results obtained from well tested MC codes. Using six GPU cards, 6x10{sup 6} histories of electrons were simulated within 2 seconds. In comparison, the same case running the EGSnrc and MCNPX codes required 1645 seconds and 9213 seconds, respectively, on a CPU with a single core used. 
 ~\cite{su2013monte}
 %

 %
 Hardware accelerators are currently becoming increasingly important in boosting high performance computing systems. In this study, we tested the performance of two accelerator models, Nvidia Tesla M2090 GPU and Intel Xeon Phi 5110p coprocessor, using a new Monte Carlo photon transport package called ARCHER-CT we have developed for fast CT imaging dose calculation. The package contains three components, ARCHER-CTCPU, ARCHER-CTGPU and ARCHER-CTCOP designed to be run on the multi-core CPU, GPU and coprocessor architectures respectively. A detailed GE LightSpeed Multi-Detector Computed Tomography (MDCT) scanner model and a family of voxel patient phantoms are included in the code to calculate absorbed dose to radiosensitive organs under user-specified scan protocols. The results from ARCHER agree well with those from the production code Monte Carlo N-Particle eXtended (MCNPX). It is found that all the code components are significantly faster than the parallel MCNPX run on 12 MPI processes, and that the GPU and coprocessor codes are 5.15?5.81 and 3.30?3.38 times faster than the parallel ARCHER-CTCPU, respectively. The M2090 GPU performs better than the 5110p coprocessor in our specific test. Besides, the heterogeneous computation mode in which the CPU and the hardware accelerator work concurrently can increase the overall performance by 13?18\%.
 ~\cite{liu2015comparison}
 %
