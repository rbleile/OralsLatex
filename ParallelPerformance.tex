\subsection{ \textbf{ Parallel Performance }}

Since the inception of Monte Carlo particle transport over sixty years ago,
there has been tremendous growth as Monte Carlo applications have adapted to maximize performance on each architecture that has evolved over time.
%
The first models developed in 1947 would take five hours to compute 100 collisions, a task that today can be done in milliseconds.
%
In the 1940's and 1950's, Monte Carlo codes were written in very low level languages on the earliest computers.
%
The 1960's to 1980's saw a great increase in the capabilities of the Monte Carlo codes as computing power increased and codes become more fully featured.
%
In the 1980's Monte Carlo codes adopted parallel/vector machines.
%
In the 1990's Monte Carlo codes become more commonplace and parallelism increased to 100s or 1000s or processors through message passing with Parallel Virutal Machines (PVM)~\cite{pvm2011} or the Message Passing Interface (MPI)~\cite{blaiseMPI}.
%
In the 2000's multicore processors meant threading became more commonplace, mixing local and global forms of parallelism reaching tens of thousands of processors~\cite{brown2011recent}.
%

This growth in computer processing can also be categorized in terms of the styles of memory accesses.
%
Early systems were shared-memory environments almost exclusively.
%
Then distributed-memory systems became popular.
%
Finally, hybrid parallel systems, meaning systems that use both distributed- and shared-memory parallelism, became popular.
%
The following discussion is organized around these three types of parallelism.

\subsection*{ \textbf{Shared-Memory Performance}} 

Shared-memory systems refer to machines or models where all processors can access the same memory space.
%
Taking this a step further, in the unified memory architecture (UMA), all processors have access to the same memory and access to all memory takes the same amount of time~\cite{el2005advanced}.
%
One type of shared-memory system that was popular was the vector machine.
%
Vector machines took the shared-memory system and added additional synchronicity to the system by making all of the processors issue the same instruction~\cite{russell1978cray}.
%

\subsection*{Vector Machine Performance}

%
In the 1980's Monte Carlo transport algorithms began adapting ``event-based" methods.
%
The traditional history-based approach was not well suited for vector architectures, since the particle histories follow independent code paths.
%
In order to vectorize the algorithms and make them usable on the vector machine architectures, the codes had to be reorganized to follow the same code paths across independently computed particle histories.
%
By changing the algorithm to follow events instead of histories, the Monte Carlo method could be used in a vector based approach~\cite{martin1986monte}.
%

%
A common element for work in this area is that the vector approach is often related to a stack data structure.
%
The challenge then becomes properly organizing particles into the right sub-stack so that calculations can be performed~\cite{brown1984monte, bobrowicz1984vectorized}.
%
Another approach is to try to use only one main particle stack and pull off only the minimum information needed to compute each of the events into sub-stacks~\cite{martin1986monte}.
%
With each of these approaches particle events determine not only how the particles are organized but also what information is needed for processing. 
%
The main drawback to the event based approach is the added time for data movement or sorting.
%

%
Brown reported the potential for speedups of 20x-85x in his theoretical analysis of the use of event-based methods, and deemed this approach well worth the efforts required to change codes around in order to use this approach on vector machines ~\cite{brown1984monte}.
%
Martin's single big stack, sub-stack approach saw speedups randing from 5x to 12x depending on the problem and the machine he was running on~\cite{martin1986monte}.
%
Bobrowicz's explicit stack approach reached speedups of around 8x - 10x compared with the original history-based approach~\cite{bobrowicz1984vectorized}.
%
Finally, Burns used GAMTEB, the LANL Benchmark code, to show he could achieve similar performance to Bobrowicz by following a similar approach as Brown~\cite{burns1989vectorization}.
%

\subsection*{Multi-Threaded Architecture Performance}

%
Other shared-memory systems, separate from vector machines, were tried in this era.
%
One such machine was the Tera Multi-Threaded Architecture (MTA).
%
This approach focused on the use of parallel processors, hardware threading, and a shared-memory no cache design.
%
The idea was to mask away memory latency by focusing on threading~\cite{majumdar2000parallel, snavely1998multi}.
%

%
Majumdar tried two methods of parallelizing the photon transport application TPHOT on the Tera MTA~\cite{majumdar2000parallel}. 
%
In TPHOT, looping occurred over spatial zones and all possible energy levels, with photons being computed when their containing zone/energy level was processed.
%
So for this application Majumdar chose to parallelize both over zones and over a combination of zones and energies through loop unrolling.
%
Table~\ref{tab:MTAPerf} shows that the parallelization on the MTA over zones and energies maintains incredible efficiency giving their application good speedups, while parallelizing over only zones does not expose enough parallel work to hide memory latency and so efficiency drops off quickly~\cite{majumdar2000parallel}.
%
\begin{table}
\caption {Parallel performance on the MTA using multithreaing~\cite{majumdar2000parallel}} \label{tab:MTAPerf} 
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Procs & Time (sec) & Speedup & Efficiency \\
\hline
\multicolumn{4}{|c|}{Parallelization by zones only} \\
\hline
1 & 764 & 1.00 & 1.00 \\
\hline
2 & 400 & 1.91 & 0.95 \\
\hline
4 & 227 & 3.37 & 0.84 \\
\hline
8 & 167 & 4.58 & 0.57 \\
\hline
\multicolumn{4}{|c|}{Parallelization by zones and energies} \\
\hline
1 & 745 & 1.00 & 1.00 \\
\hline
2 & 370 & 2.01 & 1.01 \\
\hline
4 & 187 & 3.98 & 0.99 \\
\hline
8 & 94 & 7.92 & 0.99 \\
\hline
\end{tabular}
\end{center}
\end{table}

More modern systems utilize shared-memory ideas as well, with a majority of the scientific efforts utilizing OpenMP threading models for shared-memory processing.
%
Often this model is overlooked in preference of distributed computing via MPI but that is not always the case.
%
Given an all particle method, OpenMP codes tend to scale with good efficiency with the only drawbacks having to do with possible atomic operations occurring during the collection of output tallies.
%
In the case of no atomic operations and plenty of work, shared-memory implementations have nearly perfect efficiency on a node~\cite{siegel2014multi}.

\subsection*{ \textbf{Distributed-Memory Performance}}

One of the major transitions in supercomputing history came with the shift from vector computing to distributed-memory computing.
%
This type of computing is most often done with MPI and has, for the last 20 years, been a primary method of achieving parallel performance on small scale compute clusters and large scale supercomputers alike.
%
In the message passing model of parallelism, independent processes work together through the use of messages to synchronize actions or pass data between processors.
%
In this model, parallel efficiency is generally improved by spending more time working independently and is negatively affected by time spent sending messages or idled at a synchronization point~\cite{yanghybrid}.
%

%
The Monte Carlo particle transport history-based approach lends itself to the distributed model very well.
%
Each particle history is independent of any other particle histories and can be easily split up over processors~\cite{yanghybrid}.
%
However, moving to distributed memory systems creates complications in handling large and complex geometries.
%
Domain decomposition is typically used in this case, although it increases the complexity in using message passing.
%
Domain decomposition challenges are discussed further in Section 3.B.
%

%
Given the embarrassingly parallel nature of the Monte Carlo transport problem, the performance of this model produces results as expected.
%
As MPI processes increase, Monte Carlo transport continues to get a nearly linear speedup.
%
Majumdar shows that with 16 nodes and 8 MPI tasks per node, his biggest run, he was still able to achieve a 88\% efficiency~\cite{majumdar2000parallel} in his code that was parallel over zones and energies.
%
In an all particle code, Mercury at LLNL, parallel efficiencies of \textasciitilde95\% are seen when using MPI parallelism up to 2 million processors~\cite{o2013scalable}.

\subsection*{ \textbf{Distributed- + Shared-Memory Performance}}

%
Given the heterogenous nature of today's computing environment, and even in the fairly homogenous environment that has been prevalent, it is a common next step to consider combining distributed and shared-memory parallel schemes.
%
Shared-memory parallelism exists on a node or on one of the new accelerator devices.
%
Distributed-memory parallelism provides the opportunity for scaling to large supercomputers or clusters, giving users many nodes to work with.
%
Given the natural fit between these two models it is surprising how rarely they are combined in practice.
%
Developers may have avoided using both types of parallelism to date since 
distributed-memory approaches work ``well enough" within a node, meaning
that each code on a node can be its own MPI task.
%
With the addition of accelerator architectures developers will no longer be able to ignore combining shared-memory and distributed-memory models
%

%
The combined distributed-shared model is often referred to as MPI+X~\cite{michaelwolfe2014}.
%
The X in this description being replaced with whichever shared-memory system is preferred.
%
The most common implementation of MPI+X to date is the MPI + OpenMP model.
%
In the MPI + OpenMP model, MPI is utilized for node to node communication and OpenMP is used for on node parallelism~\cite{michaelwolfe2014}.
%

%
Yang has recently shown that the MPI+OpenMP model has the benefit of achieving nearly perfect parallel efficiency and of significantly decreasing the memory overhead to an equivalent MPI only implementation.
%
He was able to show 82-84\% parallel efficiency and a decrease in memory cost from \textasciitilde1.4GB to \textasciitilde200MB for 8 processors~\cite{yanghybrid}.
%
Majumdar shows that with 16 nodes and 8 OpenMP threads per node, he was able to achieve a 95\% parallel efficiency which is an improvement over his MPI only method's 88\% parallel efficiency~\cite{majumdar2000parallel}. 
%
