\subsection{ \textbf{First Pass at GPU Computing}}

In this section we will analyze the different approaches that people have taken to get their Monte Carlo transport codes working for GPUs.
%
By comparing and contrasting these different approaches we can see for a variety of problems if any approaches were more successful and which approaches struggled to get desired levels of performance out of the hardware.
%
 It is important to note that the actual runtimes and speedups are less important then the theoretical speedups compared to the achieved speedups as hardware has been changing very quickly over the last ten years for these comparisons.
 %

\subsection*{ Accuracy: }

One of the first considerations the scientific community has when being introduced to a new computing platform is what levels of accuracy can they achieve with their simulation codes.
%
It was common that in early GPU computing when double floating point precision was not supported or supported well that people started thinking about GPU computing as not being accurate enough for their needs.
%
Many early attempts at GPU computing includes discussions of accuracy in order to validate the correctness of their results.
%
While modern GPGPUs support double precision much better than before making much of the worry irrelevant, it is still important to consider the accuracy of a method that runs on a new hardware and may use a new algorithm.

%
Nelson discussed accuracy in his thesis work ~\cite{nelson2009monte}, stating that during the time of his work the floating-point arithmetic accuracy was not fully IEEE-754 compliant which opens the question of accuracy without a more fully featured test. Additionally, since NVIDIA as complete control over the implementation of floating point calculations on their GPUs their may be differences between generations that mitigate the usefulness of an accuracy study on one generation of hardware.
%
Current generations of the NVIDIA GPU hardware are IEE-754 compliant. 
%
In order to address issues of floating point accuracy they have even included a detailed description of the standard and the way CUDA follows the standard showing that at least while floating point accuracy is still a concern it is no more a concern than it was on a CPU implementation.~\cite{cudaToolkitv7.5}
%
Nelson's other primary accuracy consideration was the difference in computation time between double and single precision.
%
In older GPU hardware there was no support for double precision in the hardware and so in order to achieve double precision significantly more calculations were needed.
%
In modern GPU hardware double precision is becoming increasingly better supported and in the gpgpu cards there are dedicated double precision units and all of the necessary hardware changes required to include them.
%

Others also wanted to consider the accuracy of their initial gpu conversions.
%
Jia et. al ~\cite{jia2010development} showed that in their development of a Mont Carlo dose calculation code they could achieve speedups of 5 to 6.6 times their CPU version while maintaining within 1\% of the dosing for more then 98\% of the calculation points.
%
They considered this adequate accuracy to consider using GPUs for doing these computations.
%
Yepes et. al~\cite{yepes2010gpu} also considered accuracy in their assessment of their GPU implementation.
%
They concluded that in term of accuracy there was a good agreement between the dose distributions calculated with each version they ran, the largest discrepencies being only $\sim$3\%, and so they could run the GPU version as accurately as any general-purpose Monte Carlo program.
%

\subsection*{Performance:}

A second factor that is important to people making their first pass at GPU Monte Carlo is performance.
%
Most early GPU studies emphasize the speedups between CPU and GPU as the primary advantage for moving over to the GPU hardware.
%
Given the change in supercomputing designs these comparisons have become increasingly more important.
%

Badal and Badano~\cite{badal2009accelerating} present work on photon transport in a voxelized geometry showing results around 27X over a single core CPU.
%
Their work emphasizes simply using GPUs instead of CPUs and the advanatage as GPUs continue to increase in performance faster than CPUs.
%

Nelsons work presented in his thesis~\cite{nelson2009monte} shows a variety of models and considerations for his performance results.
%
His work solving neutron transport considered multiple models for running the problem and optimizing for the GPU.
%
The model that produced his best results shows 19.37X from a 49,152 neutrons per batch run for single precision.
%
The same model shows 23.91X when using single precision and fast math,
%
For double precision performance the model labeled model four had the fastest speedups with 11.21X and 12.66X with fast math.

Work presented by Tickner~\cite{tickner2010monte} on X-ray and gamma ray transport uses a slightly modified scheme from the others by launching particles on a per block basis.
%
In this way he hoped to remove the instruction level dependancies between particles running on the GPU hardware.
%
In this work he showed he was capable of producing speedups of up to 35X over those running the old algorithm.

Jia et. al's  work~\cite{jia2010development} in a dose calculation code for coupled electron photon transport follows a relatively straight forward algorithm.
%
First they copy the necessary data to the GPU, then they launch with each thread independently computing the necessary work for their particle.
%
Finally, when the correct number of particles has been simulated the results are brought back to the CPU and the program terminates.
%
This method produced a modest performance increase of around 5 to 6.6X their runs on a CPU when run on a GPU.
%
The limitation of this speedup was attributed to the branching of the code and that effect it had on the GPU hardware.

In contrast to Jia et al's work Yepes et al~\cite{yepes2010gpu} showed that a slightly different algorithm could greatly improve results.
%
By converting a track-repeating algorithm instead of a full Monte Carlo Yepes et al gained aroung 75X the performance on the GPU over the CPU.
%
It is thought that the simpler logic of this algorithm generated threads which followed closer logic to that of the algorithm presented in Jia at al's work.

Throughout all of these examples one common theme can be seen.
%
Performance can be gained doing Monte Carlo on the GPU.
%
Performance can be more difficult to get due to the highly divergent nature of the full Monte Carlo application.
%
Methods to deal with this divergence can show promising results that are worthy of further study.
%
These outcomes are expected outcomes since Monte Carlo applications embarrassingly parallel (a good sign for GPUs ) but also incredibly divergent ( a bad sign for GPUs ).

\subsection*{Approaches: }

Monte Carlo transport applications tend to follow a simple model where each tracked particle is given its own thread and computations progress embarrassingly parallel. 
%
On a GPU this also makes sense as a starting point since particles are independent and this progression leads to a simple natural parallel approach.
%
It is often pointed out however that due to the divergent nature of Monte Carlo this approach might not be the best way organize Monte Carlo codes on GPU hardware.
%

We mentioned in the previous section one persons approach at solving this dilemma and their performance gains by dealing with this issue.
%
We will now look closer at the algorithm they used to better understand why if at all their approach should afford better performance than what was seen before.
%

We will first look at the particle-per-block tracking algorithm described by Tickner~\cite{tickner2010monte}.
%
First each tracked particle or quantum of radiation is given to a block of threads.
%
Then calculations are performed for one particle on each block of threads.
%
For example the particle intersection tests with the background geometry can be preformed in parallel on those threads for each piece of geometry that particle might be able to collide with.
%
Areas where these parallel instructions can be utilized within a particles calculation are then used by the threads in a block computing for that particle.
%

This particle-per-block technique has shows promise as an effective way to counteract the divergence issue.
%
Positives, as they also described, particles often diverge quite quickly from one another in the code paths they follow.
%
This means that threads in a block are not always able to travel in lock step and can cause some serialization of the parallel regions.
%
By using only one particle per block the divergence problem is nearly entirely removed from the equation.
%
Additionally this method introduces new areas of parallelism that are not otherwise being taken advantage of, instruction level parallelism in the particles calculations.
%

%
This method however, does not take full advantage of the parallelism in the hardware like those methods that do not mind the divergence do.
%
Many threads can execute simultaneously at once within a block and only groupings of 32 threads are held in a WARP forced into the lockstep pattern that causes potential slowdowns.
%
By running only one particle per block you are sacrificing some parallelism as not all tasks to calculate a particles path are parallel operations.
%
Additionally, since warps are scheduled out of thread blocks any particle operations that are not done in parallel among the threads of a block are serializing themselves in a similar manner as to those algorithms that run one thread per particle waiting while divergent particles have a turn.
%

In summary I think that this method has some merit if it can find enough parallel work in the thread block to execute additional parallel tasks that would otherwise be stalled if following a simpler method.
%
I also think that this method might end up showing the same characteristics of the simpler particle-per-thread model if the extra parallelism is not found, and instead loose out on the parallelism provided by particles that are not divergent from one another. 

A second possibly more obvious method to escape the divergence issue is to switch particle tracking algorithms more dramatically from a history based version to an event based version.
%
We will have a discussion of this further in the Section on event based algorithms later in this paper.
%
Event based approaches require much more work then simply transforming an existing code to use the history based version on the GPU.
%
And as Du et al discovered in their attempt at a event based Monte Carlo version of the Archer code~\cite{xu2015archer}~\cite{du2013evaluation}~\cite{liu2015comparison}~\cite{su2013monte}  getting any speedups with that method is a whole new host of challenges to overcome.

\subsection*{ Evaluation: }

A number of studies were conducted by groups identifying the potential benefits of GPU hardware but also the hardware and software issues when developing Monte Carlo applications.
%
Among these concerns are memory limitations, lack of ECC support, lack of software optimization, limitations of SMIP architecture, clock speeds, and complex memory allocation schemes.
%
In addition the achieved performance was often not more than could be gotten with unchanged codes on a cluster.
%
In some cases though speedups were large and easy to achieve such as the 45X speedup of the voxelized approach. 
%
The only strong conclusion from these works are that a clear and defined path are not yet known on how to take full advantage of the available parallelism without suffering performance penalties in turn. ~\cite{ding2011evaluation}

