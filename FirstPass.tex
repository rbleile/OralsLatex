\subsection{ \textbf{Monte Carlo Transport on a GPU}}

This section will analyze the different approaches that people have taken to get their Monte Carlo transport codes working for GPU architectures.
%
It will begin by comparing and contrasting different approaches by evaluating a few key areas of the studies that have been done: accuracy, performance and algorithmic choices.
%
Following will be an evaluation of the effectiveness of the approaches for the range of problems being addressed.
%
As a side note it is important to notice that speedups reported come from each paper on the hardware they were using at the time of their study; the GPU hardware has changed in computing power dramatically over the last ten years in terms of performance and additional features.

\subsection*{ \textbf{Accuracy:} }

One of the first considerations the scientific community has when being introduced to a new computing platform is what levels of accuracy can they achieve with their simulation codes.
%
Since the change from CPU to GPU computing brings a completely different hardware design it is important to understand how that design might affect the accuracy of any calculations it is performing.
%
This concern was especially important in the early days of GPU computing when double-precision was not supported and often even single-precision answers would provide slightly different results.
%
There are three key areas of accuracy that we will look at with these studies, being: Floating point precision, differences between CPU and GPU results, and IEE-754 compliance.
%

%
It was common that in the early stages of GPU computing, people would assume that GPU computing was not accurate enough for their needs.
%
Many early attempts at GPU computing include discussions of accuracy in order to validate the correctness of their results.
%
While modern GPGPUs support double-precision much better than before making much of the worry irrelevant, it is still important to consider the accuracy of a method that runs on a new hardware and may use a new algorithm.
%

\subsubsection*{\textbf{Floating Point Accuracy}}

One of the primary concerns of the early GPU studies involved understanding the limits of floating point arithmetic on the GPU architecture.
%
Nelson~\cite{nelson2009monte} describes one of his primary accuracy considerations as being the difference between single and double-precision calculations.
%
In older GPU hardware there was no support for double-precision in the hardware and so in order to achieve double-precision significantly more calculations were needed.
%
In modern GPU hardware 64 bit double-precision is becoming increasingly better supported and in the GPGPU cards there are dedicated double-precision units and all of the necessary hardware changes required to include them.
%

\subsubsection*{\textbf{Differences Between CPU and GPU results}}

Differences in results that arise when using the same precision but on different architectures are of even
larger concern than the differences between single and double-precision.
%
This concern can be explained by understanding how floating-point math is accomplished on a computer~\cite{goldberg1991every}.
%
There are two main reasons that differences arise.
%
The first is that floating point mathematical operations that are done in a different order might produce a different result and due to the nature of parallel computing often you cannot know or guarantee the order a set of calculations will be performed in.
%
The second reason is that modern day CPUs using x86 processors perform math internally on 80 bit registers while a GPU does it on 32 bit (single-precision) or 64 bit (double-precision) registers.
%
Because of this, each math operation on a CPU might stay in registers and only be rounded down to 64 bits when it is saved to memory.
%

%
Jia et al. ~\cite{jia2010development} showed that in their development of a Monte Carlo dose calculation code they could achieve speedups of 5 to 6.6 times over their CPU version while maintaining within 1\% of the dosing for more than 98\% of the calculation points.
%
They considered this adequate accuracy to consider using GPUs for doing these computations.
%
Yepes et al.~\cite{yepes2010gpu} also considered accuracy in their assessment of their GPU implementation.
%
They concluded that, in terms of accuracy, there was a good agreement between the dose distributions calculated with each version they ran, the largest discrepancies being only $\sim$3\%, and so they could run the GPU version as accurately as any general-purpose Monte Carlo program.
%
As these two groups have shown, this amount of error is often very small, and over the entire course of the simulation only brings 1-3\% errors.

\subsubsection*{\textbf{IEE-754 Compliance}}
Nelson discussed accuracy in his thesis work~\cite{nelson2009monte}, stating that during the time of his work the floating-point arithmetic accuracy was not fully IEEE-754 compliant which opens the question of accuracy without a more fully featured test. 
%
Additionally, since NVIDIA has complete control over the implementation of floating point calculations on their GPUs there may be differences between generations that mitigate the usefulness of an accuracy study on one generation of hardware.
%
Current generations of the NVIDIA GPU hardware are IEE-754 compliant however. 
%
In order to address issues of floating point accuracy they have even included a detailed description of the standard and the way CUDA follows the standard showing that at least while floating point accuracy is still a concern it is no more a concern than it was on a CPU implementation.~\cite{cudaToolkitv7.5}
%

\subsection*{\textbf{Performance:}}

A second factor that is important to people using GPU computing for Monte Carlo transport is performance.
%
Most early GPU studies emphasize their speedups over CPUs as the primary advantage for moving over to the GPU hardware.
%
Given the change in supercomputing designs these comparisons have become increasingly more important.
%

%
Often, performance is compared to the hardware maximums such as peak of FLOPS or memory bandwidth.
%
It is often assumed that an increase in available FLOPS will translate directly into incredible performance gains.
%
In Lee et al.'s Debunking the 100X GPU vs. CPU myth~\cite{lee2010debunking}, this discussion of performance is brought into new light showing the relative performance gains for different types of applications.
%
The important thing to consider is the limiting factor between the hardware and the code.
%
As a result, comparing current performance with that of peak performance is often very misleading.
%

%
The following discussions show the relative performances of Monte Carlo transport applications that either underwent a transformation to use GPUs or performed a study comparing with GPU hardware.
%
We will not see the 100x performance that is often sought after, but instead we can understand the impact that each applications problem, algorithms, and implementation differences had on the performance as a whole.
%

\subsubsection*{\textbf{Photon Transport}}
%
Badal and Badano~\cite{badal2009accelerating} present work on photon transport in a voxelized geometry showing results around 27X over a single core CPU.
%
Their work emphasizes simply using GPUs instead of CPUs as GPUs increase performance faster than CPUs.
%

%
Ren et al.~\cite{ren2010gpu} present work on photon propagation through tissue, showing around a 10X performance increase when using the GPU.
%
Their discussion expressed clearly that the performance was related strongly to the size of the data set and the number of simulated photons.
%
In addition, their results were negatively affected by high level divergence when processing different types of tissues.
%

%
Alerstam et al.~\cite{alerstam2008parallel} present work on a GPU based photon migration simulation in CUDA with speedups around 1000X over a single core CPU.
%
This specific problem does not suffer from the same divergence issues that other Monte Carlo codes have as the algorithm for completing a photon migration has very little divergence and can be easily optimized for memory layouts.
%


\subsubsection*{\textbf{Neutron Transport}}

%
Nelson's work presented in his thesis~\cite{nelson2009monte} shows a variety of models and considerations for his performance results.
%
His work solving neutron transport considered multiple models for running the problem and optimizing for the GPU.
%
The model that produced his best results shows 19X from a 49,152 neutrons per batch run for single-precision.
%
The same model shows 23X when using single-precision and fast math.
%
For double-precision performance the fastest speedups he observed were 12X.
%

Work done by Gong et al.~\cite{gong2011accelerating} in a MCNP-based application has similar performance benefits to Nelson's work.
%
Speedup factors of 16X to 23X were found depending on problem parameters.
%
This work was an introductory attempt at implementing MCNP in CUDA, since MCNP is so large it covers only a subset of possible features and problem types.
%

Heimlich et al.~\cite{heimlich2009gpu} reported a speedup of around 15X for his neutron transport application when comparing a GPU to an 8-core CPU.
%
This work focused on optimizing a history-based approach in CUDA for the GPU and using MPI+openMP for the CPU.
%
This particular algorithm contained only small amounts of divergence in the code path that computes the random walk of neutrons, providing a possibility for greater use of available parallelism.
%


\subsubsection*{\textbf{Gamma Ray Transport}}
%
Work presented by Tickner~\cite{tickner2010monte} on X-ray and gamma ray transport uses a slightly modified scheme from the others by launching particles on a per block basis.
%
In this way, he hoped to remove the instruction-level dependencies between particles running on the GPU.
%
In this work, he produced speedups of up to 35X over a single core CPU, a significant improvement over similar methods launching with a particle-per-thread scheme.
%

\subsubsection*{\textbf{Coupled Electron Photon Transport}}
%
Jia et al.'s  work~\cite{jia2010development} in a dose calculation code for coupled electron photon transport follows a relatively straight-forward algorithm.
%
In their work, they offload the data and computations to the GPU, simulate the particles, and then copy memory back.
%
This method produced a modest performance increase on a GPU of around 5 to 6.6X over their runs on a CPU.
%
The limitation of this speedup was attributed to the branching of the code.

\subsubsection*{\textbf{Track Repeating Algorithm}}
In contrast to Jia et al.'s work, Yepes et al.~\cite{yepes2010gpu} showed that a different algorithm could greatly improve results.
%
By converting a track-repeating algorithm instead of a full physics Monte Carlo code, Yepes et al. gained around 75X the performance on the GPU over the CPU.
%
It is thought that the simpler logic of this algorithm generated threads which followed less branching paths than the algorithm presented in Jia et al.'s work.

\subsubsection*{\textbf{Performance Evaluation}}
Throughout all of these examples one common theme can be seen.
%
While performance can be gained doing Monte Carlo on the GPU, it can be more difficult to get than expected due to the highly divergent nature of the Monte Carlo algorithm.
%
Methods to deal with this divergence show promising results.
%
These outcomes are expected since Monte Carlo applications are embarrassingly parallel (good for GPUs) but also incredibly divergent (bad for GPUs).
%

In this section, we have seen a wide range in performances, from as low as 5X to as high as 75X.
%
While simplifications played a large role in the 75X algorithm we do see a full Monte Carlo application achieving speeds of 35X in the case of the work by Tickner~\cite{tickner2010monte}.
%
It is important to note that while some of the differences in performance are due to the nature of each problem being solved, the algorithmic choices made can have a significant impact on the GPU implementations.
%

\subsection*{\textbf{Algorithms:}}

Based on the performance studies we have just seen, it is important to highlight the algorithmic approaches that were taken so that we can understand the performance of each approach.
%
If we can clearly find algorithms that show positive performance results, then other codes can implement them for potential gain.
%
In this section, we are going to look at a few of the important algorithms.
%

%
Monte Carlo transport applications tend to follow a simple model where each tracked particle is given its own thread and computation progresses in an embarrassingly parallel fashion. 
%
On a GPU, this also makes sense as a starting point since particles are independent and this progression leads to a natural parallel approach.
%
It is often pointed out, however, that due to the divergent nature of Monte Carlo this approach might not be the best way organize Monte Carlo codes on GPU hardware.
%

\subsubsection*{\textbf{Particle-Per-Block}}
We will first look at an alternative approach, the particle-per-block tracking algorithm described by Tickner~\cite{tickner2010monte}.
%
First each tracked particle or quantum of radiation is given to a block of threads.
%
Then calculations are performed for one particle on each block of threads.
%
For example the particle intersection tests with the background geometry can be performed in parallel on those threads for each piece of geometry that particle might be able to collide with.
%
Areas where these parallel instructions can be utilized within a particles calculation are then used by the threads in a block computing for that particle.
%

This particle-per-block technique is effective in mitigating the divergence issue.
%
Particles often diverge quickly from one another in the code paths they follow.
%
This means that threads in a block are not always able to travel in lock step and can cause some serialization of the parallel regions.
%
By using only one particle per block, the divergence problem is nearly removed from the equation.
%
Additionally, this method introduces a new area of parallelism that is not otherwise being taken advantage of: instruction-level parallelism in the calculations for a single particle.
%

%
This method, however, does not take full advantage of the parallelism in the hardware like those methods that are not sensitive to divergence.
%
Many threads can execute simultaneously within a block with potential slowdowns coming from groupings of 32 threads are held in a warp and forced into the lockstep pattern.
%
Running only one particle per block can sacrifice some parallelism, as not all tasks to calculate a particle's path are parallel operations.
%
Additionally, since warps are scheduled out of thread blocks, any particle operations that are not done in parallel among the threads of a block are serializing themselves in a similar manner as to those algorithms that run one thread per particle waiting while divergent particles have a turn.
%

In summary, this method has some merit if it can find enough parallel work in the thread block to execute additional parallel tasks that would otherwise be stalled if following a simpler method.
%
Also, this method might end up showing the same characteristics of the simpler particle-per-thread model if the extra parallelism is not found, and instead loose out on the parallelism provided by particles that are not highly divergent from one another. 

\subsubsection*{\textbf{Event-Based Approaches}}

A second, possibly more obvious method, to escape the divergence issue is to switch particle tracking algorithms more dramatically from a history based version to an event based version.
%
Section~\ref{sec:EventBased} discusses this topic later in this paper.
%
Event based approaches require much more work then simply transforming an existing code to use the history based version on the GPU.
%
And as Du et al. discovered in their attempt at an event based Monte Carlo version of the Archer code~\cite{xu2015archer}~\cite{du2013evaluation}~\cite{liu2015comparison}~\cite{su2013monte},  getting any speedups with that method creates a whole new host of challenges to overcome: divergence, atomic operations, data locality/layouts, and portability to name a few.

\subsubsection*{\textbf{Voxelization Approaches}}

This method was used for comparison on the GPUs.
%
Voxelization of a geometry is done for each voxel.
This process involved ray-stabbing numbers counted on the GPU, and then a parity-counting method was run on the CPU to detect if the voxel was inside the mesh surface~\cite{na2010deformable}.
%
This method contains no divergence since all threads follow the exact same code paths.
%
This process is often done to voxelize geometries to enable Monte Carlo codes to run.
%
Doing this algorithm with no divergence produces a 45.5x speedup on the GPU over the CPU.
%
This example is in Ding et al.'s evaluation report~\cite{ding2011evaluation} in order to show the performance of the same GPU on different aspects related to Monte Carlo transport.

\subsection*{ \textbf{Evaluation:}}

\begin{table}
\caption { GPU speedup evaluation results~\cite{ding2011evaluation} } \label{tab:GPUPerfEval} 
\begin{center}
\begin{tabular}{ |C{.10\textwidth}|C{.10\textwidth}|C{.10\textwidth}|C{.10\textwidth}|}
\hline
 & & & \\
Case & Execution Time $T_{CPU}$ (minutes) & Execution Time $T_{GPU}$ (minutes) & Speed-up Factor $T_{CPU}/T_{GPU}$ \\
 & & & \\
 \hline
 & & & \\
Neutron Transport Problem & $~ 0.496$ & $ ~0.017$ & $~ 29.2$ \\ 
 & & & \\
\hline
 & & & \\
Eigenvalue/Criticality Problem & $ 4.25 $ & $ ~ 0.5 $ & $ ~ 8.5 $ \\
 & & & \\
\hline
 & & & \\
Voxelization & $ 2380.4 $ & $ ~ 52.3 $ & $~ 45.5 $ \\
 & & & \\
\hline
\end{tabular}
\end{center}
\end{table}

A number of studies were conducted by groups identifying the potential benefits of GPU hardware but also the hardware and software issues when developing Monte Carlo applications.
%
Among these concerns are memory limitations, lack of ECC support, lack of software optimization, limitations of SIMD architecture, clock speeds, and complex memory allocation schemes.
%
In addition, the achieved performance often did not exceed that of unchanged codes on a cluster.
%
In some cases, though speedups were large and easy to achieve such as the 45X speedup of the voxelized approach. 
%
The results from Ding et al.'s evaluations can be seen in Table~\ref{tab:GPUPerfEval}.
%
The only strong conclusion from these works are that a clear and defined path are not yet known on how to take full advantage of the available parallelism without suffering performance penalties in turn. ~\cite{ding2011evaluation}
