%% Introduction Section %%
\section{ \textbf{Introduction}}

Today's supercomputer landscape is in flux.
%
Supercomputer architectures are undergoing more extreme changes now than at any point in the past twenty years.
%
An important driving factor for this change is the concern regarding power usage while scaling to larger and larger machines.
%
Modern machines are pushing up against a hard power limit, meaning that in order to increase performance they must become more power efficient.
%
As a result, architectures are transitioning from fast and complex multi-core CPUs to the more energy efficient design of larger numbers of slower and simpler processors.
%
Relative to one decade ago, the amount of parallelism available on any given node in a supercomputer is growing by factors of hundreds or thousands because of this change.
%
This transition to many-core computing brings new and interesting challenges.
%

%
While it is clear that there will be an increase in node-level parallelism, it is unclear which specific many-core architectures will be used on future supercomputing platforms.
%
There are many different architectures to choose from when designing a supercomputer, and there is currently no single consensus for a choice.
%
NVIDIA provides General Purpose Graphics Processing Units (GPGPUs) which are highly parallel throughput-optimized devices.
%
Intel provides Many Integrated Core (MIC) co-processors which provide large vector lanes and many threads.
%
Another option is Field Programmable Gate Arrays (FPGAs) which provide programmable logic circuits.
%
Across the Department of Energy (DOE) National Labs, both the NVIDIA and Intel technologies are being pursued in their newest procurements ~\cite{coralWeb, trinityWeb}.
%

Application developers now face an unclear path forward.
%
There are additional levels of complexity and potentially large changes to designing simulation codes in order to effectively utilize this increase in parallelism.
%
In addition, the factors behind supporting a new architecture are often more complex in the context of legacy codes and/or codes that aim to run effectively on many architectures.
%
Instead, the simulation code developer must address both the issue of portability and the issue of performance of their algorithms, or risk their simulation code becoming outdated to unusable very quickly.
%
This problem is especially challenging when optimizations are specific to one architecture.
%

%
Currently, there are a large number of physics and multi-physics applications that must understand how to navigate this complex and challenging landscape.
%
Simply porting to new architectures does not guarantee performance, and will still require applications to be ported individually to multiple architectures.
%
This paper will describe the current state of the art research for the Monte Carlo particle transport problem, discuss GPU usage in Monte Carlo particle transport, evaluate portable performance solutions that have been explored up to this point, and provide a proposal for future research to fill in the important gaps in knowledge.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%