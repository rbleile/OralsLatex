\subsection{Load Balance and Domain Decomposition Algorithms}

In order to achieve high levels of parallelism in transport problems with many geometries or zones different parallel execution models are used.
%
Two primary models used are domain decomposition and replication.
%
Domain decomposition involves spatial decomposition of the geometry into domains, and then the assigning of processors to work on specific domains.
%
Replication involves storing the geometry information redundantly on each processor and assigning each processor a different set of particles.
~\cite{o2005dynamic}
~\cite{procassini2005load}

Load balance is often discussed in conjunction with domain decomposition since particles often migrate between different regions of a problem and so not all spatial domains will require the same amount of computational work.
%
In many applications there is at least one portion of the calculation that must be completed by all processors before all the processors can move forward with the calculation.
%
If one processor has more work than any other all of the others must wait for that processor to complete its work~\cite{o2005dynamic}~\cite{procassini2005load}.
%
This load-imbalance can cause significant issues with scalability as paralleism is increased from hundreds to millions of processors ~\cite{o2013scalable}.
%

\subsubsection*{When to Load balance}

%
One key consideration when wanting to preform a load balance calculation is to understand the cost of preforming that calculation as well.
%
If too much time is spent making sure the problem is always perfectly load balanced then computational resources are being wasted on a non-essential calculation resulting is overall slower performance.
%
However, if too little resources are devoted to load-balancing than the problem will suffer from load-imbalance and the negative effects that entails.
%
One solution is to preform load balance at the start of each cycle or iteration of a Monte Carlo transport calculation but only when that load balance will result in a faster overall calculation~\cite{o2005dynamic}~\cite{procassini2005load}.
%

An algorithm to determine when to load balance was explained in references~\cite{o2005dynamic}~\cite{procassini2005load} where the following criterion could be checked inexpensively each cycle to determine if a load-balance operation should take place.
%
First, is to compute a speedup factor by comparing current parallel efficiency ($ \varepsilon_C $) to what parallel efficiency would be if processors were to redistribute their load ($ \varepsilon_{LB} $).
%
Second, is to predict the run time by using the time to execute the previous cycle($ \tau_{Phys} $), the speedup factor($S$), and finally the time to compute the load balance itself($ \tau_{LB} $).
%
Finally ,is to compare the predicted run with and without load balancing to determine if the operation is worth while.~\cite{o2005dynamic}
~\cite{procassini2005load}

\begin{eqnarray}
S = \frac{\varepsilon_C}{\varepsilon_{LB}} \\
\tau^{'} = \tau_{Phys} \cdot S + \tau_{LB} \\
\tau = \tau_{Phys} \\
if\ (\tau^{'} < 0.9 \cdot \tau \ )\ DynamicLoadBalance();
\end{eqnarray}

\subsubsection*{Extended Domain Decomposition}

As an extension to the domain decomposition of meshes, O'Brien and Joy demonstrated an algorithm to domain decompose Constructive Solid Geometry (CSG) in a Monte Carlo transport code.
%
One key difference between mesh and CSG geometries is that mesh geometries contain a description of cell connectivity where as cells defined though CSG do not.
%
In order to domain decompose these CSG cells each cell was given a bounding box and since each domain is also a box a test for if a cell belongs inside a domain becomes an axis-aligned box-box intersection test.~\cite{o2009domain}
%

In addition to pure mesh and pure CSG problems other combinations might be useful, such as the combination of mesh and CSG problems where there are large-scale heterogeneous and homogeneous regions.
%
In this method a mesh region is embedded inside a CSG region allowing for the use of each in whichever region one or the other is more optimal.
~\cite{greenman2009enhancements}

\subsubsection*{Load Balance at Scale}

%
When load balancing massively parallel computers it is unscalable to need to examine the workload of every processor.
%
O'Brien, Brantley and Joy present their scalable load balancing algorithm that runs in $\Theta ( log ( N ) )$ by using iterative processor-pair-wise balancing steps that will ultimitely lead to a balanced workload.
%
Their algorithm shows remarkable ability to load balance and is demonstrated up to 2 million processors on the Sequoia supercomputer at Lawrence Livermore National Laboratory.
~\cite{o2013scalable}
%

%
The pair-wise load balancing scheme maintained a high efficiency; with the load-balanced runs maintaining efficiencies of 95\% at 2 million processors
when the not load-balanced runs continuously drops to around 68\% efficiency at 2 million processors.
%
In addition the load-balanced version is able to maintain near perfect scaling up to 2 million processors.
%
By dispersing the workload over processors effectively it also decreases the overall tracking time.
~\cite{o2013scalable}
%

Algorithms that interact with the particles and geometries can also be revisited after domain decomposition is added.
%
Specifically a Global Particle Find algorithm, a test for done, and domain neighbor replication.
%
The global particle find can be solved with a simple tree search; even though building the tree is not a scalable algorithm it is fast.
%
By using MPI\_I allreduce() for the global test for done in place of a complex hand coded algorithm scalability and ease of maintenance is achieved.
%
Lastly, the domain neighbor replication algorithm ended up being very important or scalability and achieved 100\% load balance and reduced memory usage by using a recursive Euclidean GCD algorithm to build a bipartite graph between adjacent domains. ~\cite{o2015particle}
%


